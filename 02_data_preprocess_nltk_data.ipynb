{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Step 1: Load and Analyze Dataset ========== #\n",
    "\n",
    "# File paths\n",
    "train_path = \"C:/Users/nesil.bor/Desktop/Folders/master/DI725/DI725-transformer-sentiment-analysis/data/raw/train.csv\"\n",
    "test_path = \"C:/Users/nesil.bor/Desktop/Folders/master/DI725/DI725-transformer-sentiment-analysis/data/raw/test.csv\"\n",
    "\n",
    "# Load datasets\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "# Display dataset information\n",
    "print(\"Train Dataset Info:\")\n",
    "print(train_df.info())\n",
    "\n",
    "print(\"\\nTest Dataset Info:\")\n",
    "print(test_df.info())\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nTrain Dataset Sample:\")\n",
    "print(train_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Step 2: Column Selection ========== #\n",
    "\n",
    "# We only need 'customer_sentiment' and 'conversation'\n",
    "train_cleaned = train_df[['customer_sentiment', 'conversation']].copy()\n",
    "test_cleaned = test_df[['customer_sentiment', 'conversation']].copy()\n",
    "\n",
    "print(\"\\nColumns selected successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Step 3: Extract & Load NLTK Data ========== #\n",
    "\n",
    "# Path where the uploaded NLTK zip file will be extracted\n",
    "nltk_data_path = \"/mnt/data/nltk_data\"\n",
    "\n",
    "# Extract the uploaded nltk_data.zip\n",
    "nltk_zip_path = \"/mnt/data/nltk_data.zip\"  # Ensure you upload this file before running\n",
    "if os.path.exists(nltk_zip_path):\n",
    "    with zipfile.ZipFile(nltk_zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(nltk_data_path)\n",
    "    print(\"NLTK data extracted successfully!\")\n",
    "\n",
    "# Set NLTK data path\n",
    "nltk.data.path.append(nltk_data_path)\n",
    "\n",
    "# Load stopwords and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Step 4: Text Preprocessing Function ========== #\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Advanced preprocessing: lowercasing, removing punctuation, stopwords, and lemmatization.\"\"\"\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove punctuation and stopwords, then lemmatize\n",
    "    cleaned_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and word not in string.punctuation]\n",
    "    # Join tokens back into a string\n",
    "    return \" \".join(cleaned_tokens)\n",
    "\n",
    "# Apply preprocessing to conversation text\n",
    "train_cleaned['processed_conversation'] = train_cleaned['conversation'].apply(preprocess_text)\n",
    "test_cleaned['processed_conversation'] = test_cleaned['conversation'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Step 5: Verify Cleaned Data ========== #\n",
    "\n",
    "print(\"\\nPreprocessed Data Sample:\")\n",
    "print(train_cleaned[['customer_sentiment', 'processed_conversation']].head())\n",
    "\n",
    "# Save the cleaned datasets\n",
    "train_cleaned.to_csv(\"C:/Users/nesil.bor/Desktop/Folders/master/DI725/DI725-transformer-sentiment-analysis/data/processed/train_cleaned.csv\", index=False)\n",
    "test_cleaned.to_csv(\"C:/Users/nesil.bor/Desktop/Folders/master/DI725/DI725-transformer-sentiment-analysis/data/processed/test_cleaned.csv\", index=False)\n",
    "\n",
    "print(\"\\nPreprocessing complete! Cleaned datasets saved as train_cleaned.csv and test_cleaned.csv.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
