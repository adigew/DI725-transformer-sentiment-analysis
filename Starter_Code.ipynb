{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e259e7b-d05b-41b8-b596-6ecdd4866c60",
   "metadata": {},
   "source": [
    "# DI 725: Transformers and Attention-Based Deep Networks\n",
    "\n",
    "## An Assignment for Implementing Transformers in PyTorch\n",
    "\n",
    "The purpose of this notebook is to guide you through the usage of sample code.\n",
    "\n",
    "This notebook follows the baseline prepared by Andrej Karpathy, with a custom dataset (Don-Quixote by Cervantes). This version of the code, called [nanoGPT](https://github.com/karpathy/nanoGPT), is a revisit to his famous [minGPT](https://github.com/karpathy/minGPT).\n",
    "### Author:\n",
    "* Ümit Mert Çağlar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715be989-8426-4406-bd8f-2bcf0e003f09",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "Install requirements for your environment, comment out for later uses.\n",
    "\n",
    "Dependencies:\n",
    "\n",
    "- [pytorch](https://pytorch.org)\n",
    "- [numpy](https://numpy.org/install/)\n",
    "-  `transformers` for huggingface transformers (to load GPT-2 checkpoints)\n",
    "-  `datasets` for huggingface datasets (to download + preprocess datasets)\n",
    "-  `tiktoken` for OpenAI's fast BPE code\n",
    "-  `wandb` for optional logging\n",
    "-  `tqdm` for progress bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69136d40-c5ac-4623-899c-3b5ad21f368c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in c:\\users\\nesil.bor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (2.6.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\nesil.bor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (2.2.4)\n",
      "Requirement already satisfied: transformers in c:\\users\\nesil.bor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (4.49.0)\n",
      "Requirement already satisfied: datasets in c:\\users\\nesil.bor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (3.5.0)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\nesil.bor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (0.9.0)\n",
      "Requirement already satisfied: wandb in c:\\users\\nesil.bor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (0.18.7)\n",
      "Requirement already satisfied: tqdm in c:\\users\\nesil.bor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\nesil.bor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\nesil.bor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (4.13.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\nesil.bor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nesil.bor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\nesil.bor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\nesil.bor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (78.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\nesil.bor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\nesil.bor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\nesil.bor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers) (0.27.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\nesil.bor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\nesil.bor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\nesil.bor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\nesil.bor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\nesil.bor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\nesil.bor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\nesil.bor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\nesil.bor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\nesil.bor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\nesil.bor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\nesil.bor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\nesil.bor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets) (3.11.16)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in c:\\users\\nesil.bor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\nesil.bor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in c:\\users\\nesil.bor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\nesil.bor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from wandb) (4.3.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in c:\\users\\nesil.bor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from wandb) (5.29.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\nesil.bor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from wandb) (6.1.0)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in c:\\users\\nesil.bor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from wandb) (2.19.0)\n",
      "Requirement already satisfied: setproctitle in c:\\users\\nesil.bor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from wandb) (1.3.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\nesil.bor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: six>=1.4.0 in c:\\users\\nesil.bor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\nesil.bor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\nesil.bor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\nesil.bor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\nesil.bor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\nesil.bor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp->datasets) (6.3.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\nesil.bor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\nesil.bor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\nesil.bor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nesil.bor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nesil.bor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nesil.bor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nesil.bor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nesil.bor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\nesil.bor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nesil.bor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\nesil.bor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\nesil.bor\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
     ]
    }
   ],
   "source": [
    "#!pip install numpy transformers datasets tiktoken wandb tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01777420",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip uninstall torch\n",
    "#!pip install torch --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0aa90f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu121\n",
      "True\n",
      "12.1\n",
      "0\n",
      "NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e72d12-9aa6-456f-ae34-2c52aaeee7c3",
   "metadata": {},
   "source": [
    "The fastest way to get started to transformers, apart from following the labs of DI725, is to use a small model and dataset. For this purpose, we will start with training a character-level GPT on the Don-Quixote by Cervantes. The code will download a single file (2MB) and apply some transformations. Examine the code [prepare.py](data/don_char/prepare.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa2cade-4742-4b44-bcb2-0ae72c9571ad",
   "metadata": {},
   "source": [
    "## Quick Start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6869a547-0ebb-4be6-9b25-f158db64407e",
   "metadata": {},
   "source": [
    "Use the following to prepare the don-quixote novel treated in character level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17a08a93-5556-4cd9-ad2d-cc58d0363d52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train class distribution:\n",
      "sentiment\n",
      "1    542\n",
      "0    411\n",
      "2     17\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test class distribution:\n",
      "sentiment\n",
      "0    10\n",
      "1    10\n",
      "2    10\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Data processing complete:\n",
      "- Training samples: 776\n",
      "- Validation samples: 194\n",
      "- Test samples: 30\n",
      "Data saved to: c:\\Users\\nesil.bor\\Desktop\\Folders\\master\\DI725\\DI725-transformer-sentiment-analysis\\data\\sentiment\\processed\n",
      "\n",
      "Verifying saved files:\n",
      "- train.bin: 794624 bytes\n",
      "- val.bin: 198656 bytes\n",
      "- test.bin: 30720 bytes\n",
      "- train_labels.pkl: 928 bytes\n",
      "- val_labels.pkl: 342 bytes\n",
      "- test_labels.pkl: 178 bytes\n"
     ]
    }
   ],
   "source": [
    "!python data/sentiment/prepare.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a34276-d844-435d-9e16-12f567969d5f",
   "metadata": {},
   "source": [
    "This creates a `train.bin` and `val.bin` in that data directory. Now it is time to train our own GPT. The size of the GPT model depends on the computational resources. It is advised to have a GPU for heavy works, and to train lightweight and evaluate and infer models with a CPU.\n",
    "\n",
    "Small scale GPT with the settings provided in the [config/train_don_char.py](config/train_don_char.py) config file will be trained with the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ad3097d-65d8-4a72-a8d0-b5fa463b49c8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loaded train dataset: 776 samples\n",
      "Loaded val dataset: 194 samples\n",
      "DataLoaders initialized\n",
      "W&B initialized in online mode. View live at: https://wandb.ai/<your-username>/nanoGPT-sentiment\n",
      "Model moved to device\n",
      "Optimizer initialized\n",
      "Starting training loop\n",
      "Iter 0, Loss: 1.2155, Accuracy: 0.3750, Data: 0.023s, Forward: 0.184s, Backward: 0.211s, Total: 0.418s\n",
      "Validation Loss: 1.2600, Validation Accuracy: 0.4250\n",
      "Iter 50, Loss: 0.6942, Accuracy: 0.6875, Data: 0.001s, Forward: 0.003s, Backward: 0.121s, Total: 0.124s\n",
      "Iter 100, Loss: 0.2954, Accuracy: 0.8750, Data: 0.001s, Forward: 0.001s, Backward: 0.124s, Total: 0.126s\n",
      "Validation Loss: 0.4609, Validation Accuracy: 0.8313\n",
      "Iter 150, Loss: 0.1122, Accuracy: 1.0000, Data: 0.000s, Forward: 0.004s, Backward: 0.122s, Total: 0.126s\n",
      "Iter 200, Loss: 0.6257, Accuracy: 0.8750, Data: 0.000s, Forward: 0.005s, Backward: 0.121s, Total: 0.126s\n",
      "Validation Loss: 0.7716, Validation Accuracy: 0.8063\n",
      "Patience counter: 1/2\n",
      "Iter 250, Loss: 0.2014, Accuracy: 0.9375, Data: 0.000s, Forward: 0.005s, Backward: 0.122s, Total: 0.127s\n",
      "Iter 300, Loss: 0.0244, Accuracy: 1.0000, Data: 0.001s, Forward: 0.004s, Backward: 0.123s, Total: 0.128s\n",
      "Validation Loss: 0.4561, Validation Accuracy: 0.8812\n",
      "Iter 350, Loss: 0.1412, Accuracy: 0.9375, Data: 0.002s, Forward: 0.004s, Backward: 0.123s, Total: 0.129s\n",
      "Iter 400, Loss: 0.0051, Accuracy: 1.0000, Data: 0.000s, Forward: 0.000s, Backward: 0.129s, Total: 0.129s\n",
      "Validation Loss: 0.5456, Validation Accuracy: 0.8938\n",
      "Patience counter: 1/2\n",
      "Iter 450, Loss: 0.0167, Accuracy: 1.0000, Data: 0.001s, Forward: 0.005s, Backward: 0.121s, Total: 0.127s\n",
      "Iter 500, Loss: 0.0613, Accuracy: 0.9375, Data: 0.000s, Forward: 0.005s, Backward: 0.123s, Total: 0.128s\n",
      "Validation Loss: 0.3279, Validation Accuracy: 0.9125\n",
      "Iter 550, Loss: 0.0017, Accuracy: 1.0000, Data: 0.000s, Forward: 0.005s, Backward: 0.120s, Total: 0.125s\n",
      "Iter 600, Loss: 0.0021, Accuracy: 1.0000, Data: 0.000s, Forward: 0.009s, Backward: 0.127s, Total: 0.137s\n",
      "Validation Loss: 0.3502, Validation Accuracy: 0.9187\n",
      "Patience counter: 1/2\n",
      "Iter 650, Loss: 0.1359, Accuracy: 0.9375, Data: 0.000s, Forward: 0.006s, Backward: 0.124s, Total: 0.129s\n",
      "Iter 700, Loss: 0.0018, Accuracy: 1.0000, Data: 0.000s, Forward: 0.007s, Backward: 0.127s, Total: 0.133s\n",
      "Validation Loss: 0.3776, Validation Accuracy: 0.9313\n",
      "Patience counter: 2/2\n",
      "Early stopping triggered. Stopping training.\n",
      "Training complete. View results at: https://wandb.ai/<your-username>/nanoGPT-sentiment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: adigew (adigew-middle-east-technical-university). Use `wandb login --relogin` to force relogin\n",
      "wandb: Tracking run with wandb version 0.18.7\n",
      "wandb: Run data is saved locally in c:\\Users\\nesil.bor\\Desktop\\Folders\\master\\DI725\\DI725-transformer-sentiment-analysis\\wandb\\run-20250404_203501-lfbbvnw3\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run neat-eon-22\n",
      "wandb:  View project at https://wandb.ai/adigew-middle-east-technical-university/nanoGPT-sentiment\n",
      "wandb:  View run at https://wandb.ai/adigew-middle-east-technical-university/nanoGPT-sentiment/runs/lfbbvnw3\n",
      "wandb:                                                                                \n",
      "wandb: \n",
      "wandb: Run history:\n",
      "wandb:  backward_time █▁▁▁▁▁▁▁▂▁▁▁▂▁▂\n",
      "wandb:      data_time █▁▁▁▁▁▁▂▁▁▁▁▁▁▁\n",
      "wandb:   forward_time █▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "wandb:      iteration ▁▁▁▂▂▃▃▃▃▄▄▅▅▅▅▆▆▇▇▇▇██\n",
      "wandb:  learning_rate ▁▅████▇▇▆▆▅▅▄▄▃\n",
      "wandb: train_accuracy ▁▅▇█▇▇█▇██▇██▇█\n",
      "wandb:     train_loss █▅▃▂▅▂▁▂▁▁▁▁▁▂▁\n",
      "wandb:   val_accuracy ▁▇▆▇▇███\n",
      "wandb:       val_loss █▂▄▂▃▁▁▁\n",
      "wandb: \n",
      "wandb: Run summary:\n",
      "wandb:  backward_time 0.1267\n",
      "wandb:      data_time 0\n",
      "wandb:   forward_time 0.00666\n",
      "wandb:      iteration 700\n",
      "wandb:  learning_rate 6e-05\n",
      "wandb: train_accuracy 1\n",
      "wandb:     train_loss 0.00182\n",
      "wandb:   val_accuracy 0.93125\n",
      "wandb:       val_loss 0.37757\n",
      "wandb: \n",
      "wandb:  View run neat-eon-22 at: https://wandb.ai/adigew-middle-east-technical-university/nanoGPT-sentiment/runs/lfbbvnw3\n",
      "wandb:  View project at: https://wandb.ai/adigew-middle-east-technical-university/nanoGPT-sentiment\n",
      "wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "wandb: Find logs at: .\\wandb\\run-20250404_203501-lfbbvnw3\\logs\n"
     ]
    }
   ],
   "source": [
    "!python train.py --config=config/train_sentiment.py --compile=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a06da9f-a5a1-4621-806c-adad9b2d98d5",
   "metadata": {},
   "source": [
    "We are training a small scaled GPT with a context size of up to 256 characters, 384 feature channels, 6 layers of transformer with 6 attention heads. On one GTX 3070 GPU this training run takes about 10 minutes and the best validation loss is 1.1620. Based on the configuration, the model checkpoints are being written into the `--out_dir` directory `out-don-char`. So once the training finishes we can sample from the best model by pointing the sampling script at this directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d3380c5-976e-4dbb-b5dc-08ba56f0d93c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from out-sentiment\\best_model.pt and moved to cuda\n",
      "Loaded test dataset: 30 samples\n",
      "\n",
      "Sample 1:\n",
      "Text: thank you for calling brownbox customer support  my name is sarah  how may i assist you today? hi  s...\n",
      "Predicted Sentiment: Neutral (Probabilities: [0.014343023300170898, 0.9666900038719177, 0.018967006355524063])\n",
      "True Sentiment: Negative\n",
      "\n",
      "Sample 2:\n",
      "Text: thank you for calling brownbox customer support  my name is sarah  how can i assist you today? hi sa...\n",
      "Predicted Sentiment: Negative (Probabilities: [0.9675621390342712, 0.01856013759970665, 0.013877768069505692])\n",
      "True Sentiment: Negative\n",
      "\n",
      "Sample 3:\n",
      "Text: thank you for calling brownbox customer support  my name is jane  how may i assist you today? hi jan...\n",
      "Predicted Sentiment: Negative (Probabilities: [0.9681538939476013, 0.014998635277152061, 0.016847506165504456])\n",
      "True Sentiment: Negative\n",
      "\n",
      "Sample 4:\n",
      "Text: thank you for calling brownbox customer support  my name is sarah  how may i assist you today? hi sa...\n",
      "Predicted Sentiment: Negative (Probabilities: [0.9677400588989258, 0.01380248460918665, 0.018457522615790367])\n",
      "True Sentiment: Negative\n",
      "\n",
      "Sample 5:\n",
      "Text: thank you for calling brownbox customer support  my name is sarah  how may i assist you today? hi sa...\n",
      "Predicted Sentiment: Negative (Probabilities: [0.9659473896026611, 0.02338007465004921, 0.010672609321773052])\n",
      "True Sentiment: Negative\n",
      "\n",
      "Sample 6:\n",
      "Text: thank you for calling brownbox customer support  my name is rachel  how may i assist you today? hi r...\n",
      "Predicted Sentiment: Negative (Probabilities: [0.9684048891067505, 0.015901243314146996, 0.01569392718374729])\n",
      "True Sentiment: Negative\n",
      "\n",
      "Sample 7:\n",
      "Text: thank you for calling brownbox customer support  my name is john  how may i assist you today? hi joh...\n",
      "Predicted Sentiment: Negative (Probabilities: [0.9674580097198486, 0.0187771487981081, 0.013764815405011177])\n",
      "True Sentiment: Negative\n",
      "\n",
      "Sample 8:\n",
      "Text: thank you for calling brownbox customer support  my name is alex  how may i assist you today? hi  al...\n",
      "Predicted Sentiment: Negative (Probabilities: [0.967836320400238, 0.016475636512041092, 0.015688026323914528])\n",
      "True Sentiment: Negative\n",
      "\n",
      "Sample 9:\n",
      "Text: hello  thank you for contacting brownbox customer support  how may i assist you today? hi  i am extr...\n",
      "Predicted Sentiment: Negative (Probabilities: [0.9692482352256775, 0.016512494534254074, 0.014239216223359108])\n",
      "True Sentiment: Negative\n",
      "\n",
      "Sample 10:\n",
      "Text: thank you for calling brownbox customer support  my name is sarah  how may i assist you today? hi sa...\n",
      "Predicted Sentiment: Negative (Probabilities: [0.9668374061584473, 0.021341143175959587, 0.011821426451206207])\n",
      "True Sentiment: Negative\n",
      "\n",
      "Sample 11:\n",
      "Text: hello  i was looking to buy a vacuum cleaner from your website  but the one i want is currently out ...\n",
      "Predicted Sentiment: Negative (Probabilities: [0.9649214148521423, 0.022164858877658844, 0.012913726270198822])\n",
      "True Sentiment: Neutral\n",
      "\n",
      "Sample 12:\n",
      "Text: thank you for calling brownbox customer support  my name is sarah  how may i assist you today? hi sa...\n",
      "Predicted Sentiment: Neutral (Probabilities: [0.21488188207149506, 0.7751970291137695, 0.009921101853251457])\n",
      "True Sentiment: Neutral\n",
      "\n",
      "Sample 13:\n",
      "Text: thank you for calling brownbox customer support  my name is emily  how may i assist you today? hi  e...\n",
      "Predicted Sentiment: Neutral (Probabilities: [0.012044432573020458, 0.9645727872848511, 0.02338281087577343])\n",
      "True Sentiment: Neutral\n",
      "\n",
      "Sample 14:\n",
      "Text: thank you for contacting brownbox customer support  my name is john  how can i assist you today? hi ...\n",
      "Predicted Sentiment: Negative (Probabilities: [0.5884491205215454, 0.40249398350715637, 0.009056853130459785])\n",
      "True Sentiment: Neutral\n",
      "\n",
      "Sample 15:\n",
      "Text: hi  i have a question about return checks and fees for a vacuum cleaner i returned  hello thank you ...\n",
      "Predicted Sentiment: Neutral (Probabilities: [0.009507699869573116, 0.9383330345153809, 0.052159324288368225])\n",
      "True Sentiment: Neutral\n",
      "\n",
      "Sample 16:\n",
      "Text: hello  thank you for calling brownbox customer support  my name is john  how may i assist you today?...\n",
      "Predicted Sentiment: Neutral (Probabilities: [0.012607245706021786, 0.96037358045578, 0.02701917663216591])\n",
      "True Sentiment: Neutral\n",
      "\n",
      "Sample 17:\n",
      "Text: thank you for reaching out to brownbox customer support  how may i assist you today? hi  i received ...\n",
      "Predicted Sentiment: Neutral (Probabilities: [0.009854378178715706, 0.9522823095321655, 0.03786338493227959])\n",
      "True Sentiment: Neutral\n",
      "\n",
      "Sample 18:\n",
      "Text: hello  thank you for calling brownbox customer support  my name is sarah  how may i assist you today...\n",
      "Predicted Sentiment: Neutral (Probabilities: [0.012118501588702202, 0.9611547589302063, 0.026726752519607544])\n",
      "True Sentiment: Neutral\n",
      "\n",
      "Sample 19:\n",
      "Text: thank you for calling brownbox customer support  my name is rachel  how may i assist you today? hi  ...\n",
      "Predicted Sentiment: Neutral (Probabilities: [0.014383320696651936, 0.9655731320381165, 0.020043494179844856])\n",
      "True Sentiment: Neutral\n",
      "\n",
      "Sample 20:\n",
      "Text: thank you for contacting brownbox customer support  my name is rachel  how may i assist you today? h...\n",
      "Predicted Sentiment: Positive (Probabilities: [0.013944830745458603, 0.3356749415397644, 0.6503801941871643])\n",
      "True Sentiment: Neutral\n",
      "\n",
      "Sample 21:\n",
      "Text: thank you for calling brownbox customer support  my name is sarah  how can i assist you today? hi sa...\n",
      "Predicted Sentiment: Neutral (Probabilities: [0.013115031644701958, 0.6537413001060486, 0.3331436514854431])\n",
      "True Sentiment: Positive\n",
      "\n",
      "Sample 22:\n",
      "Text: thank you for calling brownbox customer support  my name is emily  how may i assist you today? hi em...\n",
      "Predicted Sentiment: Neutral (Probabilities: [0.010757780633866787, 0.7822187542915344, 0.20702342689037323])\n",
      "True Sentiment: Positive\n",
      "\n",
      "Sample 23:\n",
      "Text: thank you for calling brownbox customer support  my name is alex  how may i assist you today? hi ale...\n",
      "Predicted Sentiment: Neutral (Probabilities: [0.00927333626896143, 0.7832661271095276, 0.20746049284934998])\n",
      "True Sentiment: Positive\n",
      "\n",
      "Sample 24:\n",
      "Text: thank you for calling brownbox customer support  my name is sarah  how can i assist you today? hi sa...\n",
      "Predicted Sentiment: Neutral (Probabilities: [0.009479166939854622, 0.8296710252761841, 0.16084976494312286])\n",
      "True Sentiment: Positive\n",
      "\n",
      "Sample 25:\n",
      "Text: hi  i placed an order for a vacuum cleaner yesterday  and i just wanted to confirm the order status ...\n",
      "Predicted Sentiment: Positive (Probabilities: [0.03107735700905323, 0.10802832990884781, 0.8608942627906799])\n",
      "True Sentiment: Positive\n",
      "\n",
      "Sample 26:\n",
      "Text: hi  i'm calling to inquire about my order status for a smart band i purchased from brownbox  hello t...\n",
      "Predicted Sentiment: Neutral (Probabilities: [0.013210558332502842, 0.7325469851493835, 0.2542424499988556])\n",
      "True Sentiment: Positive\n",
      "\n",
      "Sample 27:\n",
      "Text: thank you for calling brownbox customer support  my name is john  how may i assist you today? hi joh...\n",
      "Predicted Sentiment: Neutral (Probabilities: [0.010288374498486519, 0.6705073714256287, 0.3192042112350464])\n",
      "True Sentiment: Positive\n",
      "\n",
      "Sample 28:\n",
      "Text: hi there  i'm interested in purchasing a dslr camera from your website  but i'm having trouble findi...\n",
      "Predicted Sentiment: Neutral (Probabilities: [0.009211595170199871, 0.8274405598640442, 0.16334787011146545])\n",
      "True Sentiment: Positive\n",
      "\n",
      "Sample 29:\n",
      "Text: thank you for calling brownbox customer support  my name is sarah  how may i assist you today? hi sa...\n",
      "Predicted Sentiment: Neutral (Probabilities: [0.012707988731563091, 0.6062231659889221, 0.3810688257217407])\n",
      "True Sentiment: Positive\n",
      "\n",
      "Sample 30:\n",
      "Text: thank you for calling brownbox customer support  this is john  how may i assist you today? hi john  ...\n",
      "Predicted Sentiment: Neutral (Probabilities: [0.009586544707417488, 0.9364312291145325, 0.05398227646946907])\n",
      "True Sentiment: Positive\n",
      "\n",
      "Summary: 17/30 correct, Test Accuracy: 0.5667 (56.67%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nesil.bor\\Desktop\\Folders\\master\\DI725\\DI725-transformer-sentiment-analysis\\sample1.py:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    }
   ],
   "source": [
    "!python sample.py --out_dir=out-sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b5d51e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from out-sentiment\\final_model.pt and moved to cuda\n",
      "Loaded test dataset: 30 samples\n",
      "\n",
      "Sample 1:\n",
      "Text: thank you for calling brownbox customer support  my name is sarah  how may i assist you today? hi  s...\n",
      "Predicted Sentiment: Neutral (Probabilities: [0.021121535450220108, 0.9692690968513489, 0.009609431959688663])\n",
      "True Sentiment: Negative\n",
      "\n",
      "Sample 2:\n",
      "Text: thank you for calling brownbox customer support  my name is sarah  how can i assist you today? hi sa...\n",
      "Predicted Sentiment: Negative (Probabilities: [0.9722861647605896, 0.014958624728024006, 0.012755151838064194])\n",
      "True Sentiment: Negative\n",
      "\n",
      "Sample 3:\n",
      "Text: thank you for calling brownbox customer support  my name is jane  how may i assist you today? hi jan...\n",
      "Predicted Sentiment: Negative (Probabilities: [0.9721629023551941, 0.013804925605654716, 0.01403222605586052])\n",
      "True Sentiment: Negative\n",
      "\n",
      "Sample 4:\n",
      "Text: thank you for calling brownbox customer support  my name is sarah  how may i assist you today? hi sa...\n",
      "Predicted Sentiment: Negative (Probabilities: [0.972852885723114, 0.012265088967978954, 0.014881979674100876])\n",
      "True Sentiment: Negative\n",
      "\n",
      "Sample 5:\n",
      "Text: thank you for calling brownbox customer support  my name is sarah  how may i assist you today? hi sa...\n",
      "Predicted Sentiment: Negative (Probabilities: [0.9711194038391113, 0.019059134647250175, 0.009821501560509205])\n",
      "True Sentiment: Negative\n",
      "\n",
      "Sample 6:\n",
      "Text: thank you for calling brownbox customer support  my name is rachel  how may i assist you today? hi r...\n",
      "Predicted Sentiment: Negative (Probabilities: [0.9723978638648987, 0.013634459115564823, 0.013967698439955711])\n",
      "True Sentiment: Negative\n",
      "\n",
      "Sample 7:\n",
      "Text: thank you for calling brownbox customer support  my name is john  how may i assist you today? hi joh...\n",
      "Predicted Sentiment: Negative (Probabilities: [0.9709190130233765, 0.017280759289860725, 0.011800277978181839])\n",
      "True Sentiment: Negative\n",
      "\n",
      "Sample 8:\n",
      "Text: thank you for calling brownbox customer support  my name is alex  how may i assist you today? hi  al...\n",
      "Predicted Sentiment: Negative (Probabilities: [0.9721490740776062, 0.014803759753704071, 0.013047182001173496])\n",
      "True Sentiment: Negative\n",
      "\n",
      "Sample 9:\n",
      "Text: hello  thank you for contacting brownbox customer support  how may i assist you today? hi  i am extr...\n",
      "Predicted Sentiment: Negative (Probabilities: [0.9727766513824463, 0.014852874912321568, 0.012370445765554905])\n",
      "True Sentiment: Negative\n",
      "\n",
      "Sample 10:\n",
      "Text: thank you for calling brownbox customer support  my name is sarah  how may i assist you today? hi sa...\n",
      "Predicted Sentiment: Negative (Probabilities: [0.9721810221672058, 0.016411172226071358, 0.011407794430851936])\n",
      "True Sentiment: Negative\n",
      "\n",
      "Sample 11:\n",
      "Text: hello  i was looking to buy a vacuum cleaner from your website  but the one i want is currently out ...\n",
      "Predicted Sentiment: Negative (Probabilities: [0.9713521599769592, 0.0177485179156065, 0.010899403132498264])\n",
      "True Sentiment: Neutral\n",
      "\n",
      "Sample 12:\n",
      "Text: thank you for calling brownbox customer support  my name is sarah  how may i assist you today? hi sa...\n",
      "Predicted Sentiment: Negative (Probabilities: [0.598780632019043, 0.3930101692676544, 0.008209219202399254])\n",
      "True Sentiment: Neutral\n",
      "\n",
      "Sample 13:\n",
      "Text: thank you for calling brownbox customer support  my name is emily  how may i assist you today? hi  e...\n",
      "Predicted Sentiment: Neutral (Probabilities: [0.012473350390791893, 0.9498487114906311, 0.03767797723412514])\n",
      "True Sentiment: Neutral\n",
      "\n",
      "Sample 14:\n",
      "Text: thank you for contacting brownbox customer support  my name is john  how can i assist you today? hi ...\n",
      "Predicted Sentiment: Negative (Probabilities: [0.8204224109649658, 0.17216452956199646, 0.0074130576103925705])\n",
      "True Sentiment: Neutral\n",
      "\n",
      "Sample 15:\n",
      "Text: hi  i have a question about return checks and fees for a vacuum cleaner i returned  hello thank you ...\n",
      "Predicted Sentiment: Neutral (Probabilities: [0.009847376495599747, 0.9660844802856445, 0.024068113416433334])\n",
      "True Sentiment: Neutral\n",
      "\n",
      "Sample 16:\n",
      "Text: hello  thank you for calling brownbox customer support  my name is john  how may i assist you today?...\n",
      "Predicted Sentiment: Neutral (Probabilities: [0.015107812359929085, 0.9720813632011414, 0.012810845859348774])\n",
      "True Sentiment: Neutral\n",
      "\n",
      "Sample 17:\n",
      "Text: thank you for reaching out to brownbox customer support  how may i assist you today? hi  i received ...\n",
      "Predicted Sentiment: Neutral (Probabilities: [0.008577778935432434, 0.9580582976341248, 0.033363889902830124])\n",
      "True Sentiment: Neutral\n",
      "\n",
      "Sample 18:\n",
      "Text: hello  thank you for calling brownbox customer support  my name is sarah  how may i assist you today...\n",
      "Predicted Sentiment: Neutral (Probabilities: [0.0176861435174942, 0.971189558506012, 0.011124297976493835])\n",
      "True Sentiment: Neutral\n",
      "\n",
      "Sample 19:\n",
      "Text: thank you for calling brownbox customer support  my name is rachel  how may i assist you today? hi  ...\n",
      "Predicted Sentiment: Neutral (Probabilities: [0.017615124583244324, 0.9714636206626892, 0.010921203531324863])\n",
      "True Sentiment: Neutral\n",
      "\n",
      "Sample 20:\n",
      "Text: thank you for contacting brownbox customer support  my name is rachel  how may i assist you today? h...\n",
      "Predicted Sentiment: Positive (Probabilities: [0.0137924924492836, 0.3823683559894562, 0.603839099407196])\n",
      "True Sentiment: Neutral\n",
      "\n",
      "Sample 21:\n",
      "Text: thank you for calling brownbox customer support  my name is sarah  how can i assist you today? hi sa...\n",
      "Predicted Sentiment: Neutral (Probabilities: [0.011881017126142979, 0.9141932725906372, 0.07392578572034836])\n",
      "True Sentiment: Positive\n",
      "\n",
      "Sample 22:\n",
      "Text: thank you for calling brownbox customer support  my name is emily  how may i assist you today? hi em...\n",
      "Predicted Sentiment: Neutral (Probabilities: [0.0104841822758317, 0.8431772589683533, 0.14633852243423462])\n",
      "True Sentiment: Positive\n",
      "\n",
      "Sample 23:\n",
      "Text: thank you for calling brownbox customer support  my name is alex  how may i assist you today? hi ale...\n",
      "Predicted Sentiment: Positive (Probabilities: [0.011913419701159, 0.28075897693634033, 0.7073276042938232])\n",
      "True Sentiment: Positive\n",
      "\n",
      "Sample 24:\n",
      "Text: thank you for calling brownbox customer support  my name is sarah  how can i assist you today? hi sa...\n",
      "Predicted Sentiment: Neutral (Probabilities: [0.009638515301048756, 0.8906351923942566, 0.09972625225782394])\n",
      "True Sentiment: Positive\n",
      "\n",
      "Sample 25:\n",
      "Text: hi  i placed an order for a vacuum cleaner yesterday  and i just wanted to confirm the order status ...\n",
      "Predicted Sentiment: Positive (Probabilities: [0.03850914165377617, 0.23096208274364471, 0.7305287718772888])\n",
      "True Sentiment: Positive\n",
      "\n",
      "Sample 26:\n",
      "Text: hi  i'm calling to inquire about my order status for a smart band i purchased from brownbox  hello t...\n",
      "Predicted Sentiment: Neutral (Probabilities: [0.010958738625049591, 0.8338408470153809, 0.15520046651363373])\n",
      "True Sentiment: Positive\n",
      "\n",
      "Sample 27:\n",
      "Text: thank you for calling brownbox customer support  my name is john  how may i assist you today? hi joh...\n",
      "Predicted Sentiment: Positive (Probabilities: [0.013164668343961239, 0.1933564692735672, 0.7934789061546326])\n",
      "True Sentiment: Positive\n",
      "\n",
      "Sample 28:\n",
      "Text: hi there  i'm interested in purchasing a dslr camera from your website  but i'm having trouble findi...\n",
      "Predicted Sentiment: Positive (Probabilities: [0.01256250124424696, 0.45124536752700806, 0.5361921191215515])\n",
      "True Sentiment: Positive\n",
      "\n",
      "Sample 29:\n",
      "Text: thank you for calling brownbox customer support  my name is sarah  how may i assist you today? hi sa...\n",
      "Predicted Sentiment: Positive (Probabilities: [0.014021523296833038, 0.41733318567276, 0.5686452984809875])\n",
      "True Sentiment: Positive\n",
      "\n",
      "Sample 30:\n",
      "Text: thank you for calling brownbox customer support  this is john  how may i assist you today? hi john  ...\n",
      "Predicted Sentiment: Neutral (Probabilities: [0.009082639589905739, 0.9250823855400085, 0.06583504378795624])\n",
      "True Sentiment: Positive\n",
      "\n",
      "Summary: 20/30 correct, Test Accuracy: 0.6667 (66.67%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nesil.bor\\Desktop\\Folders\\master\\DI725\\DI725-transformer-sentiment-analysis\\sample1.py:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    }
   ],
   "source": [
    "!python sample.py --out_dir=out-sentiment --checkpoint=final_model.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43535f2-8e3f-4823-955d-a30c9ed9e0eb",
   "metadata": {
    "tags": []
   },
   "source": [
    "This generates a few samples, for example:\n",
    "\n",
    "```\n",
    "“I grant all that,” said the governor; “it’s not in a low voice\n",
    "\n",
    "but not yet forget that there’s none of it the poor in the world; I’ll\n",
    "\n",
    "like to take special to have been no one to write out the stone of\n",
    "\n",
    "patience to the village.”\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61966bc-558b-4964-8575-1046d0aa6a91",
   "metadata": {},
   "source": [
    "It is pretty nice to have a GPT in a few minutes of character level training! Better results can be achieved possibly by hyperparameter tuning and finetuning (transfer learning) from a pre-trained model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b323fd-20ba-4e6b-8a5a-e40b9fdd0bca",
   "metadata": {},
   "source": [
    "## Quick start with less resources\n",
    "\n",
    "If we are [low on resources](https://www.youtube.com/watch?v=rcXzn6xXdIc), we can use a simpler version of the training, first we need to set compile to false, this is also a must for Windows OS for now. We also set the device to CPU. The model that is trained in 10 minutes for a starter grade GPU, will be trained in a much longer time, so we can also decrease the dimensions of our model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ef08093-984e-4fae-a1f9-fdf6bf4b1bd5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loaded train dataset: 776 samples\n",
      "Loaded val dataset: 194 samples\n",
      "DataLoaders initialized\n",
      "W&B initialized in online mode. View live at: https://wandb.ai/<your-username>/nanoGPT-sentiment\n",
      "Model moved to device\n",
      "Optimizer initialized\n",
      "Starting training loop\n",
      "Iter 0, Loss: 1.2171, Accuracy: 0.1875, Data: 0.023s, Forward: 0.187s, Backward: 0.183s, Total: 0.393s\n",
      "Validation Loss: 1.3223, Validation Accuracy: 0.0437\n",
      "Iter 50, Loss: 0.8911, Accuracy: 0.6250, Data: 0.000s, Forward: 0.005s, Backward: 0.121s, Total: 0.126s\n",
      "Iter 100, Loss: 0.1665, Accuracy: 0.9375, Data: 0.000s, Forward: 0.009s, Backward: 0.121s, Total: 0.130s\n",
      "Validation Loss: 0.6461, Validation Accuracy: 0.7875\n",
      "Iter 150, Loss: 0.3465, Accuracy: 0.9375, Data: 0.001s, Forward: 0.004s, Backward: 0.119s, Total: 0.124s\n",
      "Iter 200, Loss: 0.6365, Accuracy: 0.8125, Data: 0.000s, Forward: 0.002s, Backward: 0.120s, Total: 0.122s\n",
      "Validation Loss: 0.4342, Validation Accuracy: 0.8688\n",
      "Iter 250, Loss: 0.6398, Accuracy: 0.8125, Data: 0.000s, Forward: 0.003s, Backward: 0.120s, Total: 0.123s\n",
      "Iter 300, Loss: 0.3455, Accuracy: 0.8750, Data: 0.001s, Forward: 0.005s, Backward: 0.118s, Total: 0.124s\n",
      "Validation Loss: 0.5224, Validation Accuracy: 0.8812\n",
      "Patience counter: 1/2\n",
      "Iter 350, Loss: 0.0279, Accuracy: 1.0000, Data: 0.000s, Forward: 0.004s, Backward: 0.123s, Total: 0.127s\n",
      "Iter 400, Loss: 0.3279, Accuracy: 0.8750, Data: 0.000s, Forward: 0.005s, Backward: 0.117s, Total: 0.123s\n",
      "Validation Loss: 0.6644, Validation Accuracy: 0.8875\n",
      "Patience counter: 2/2\n",
      "Early stopping triggered. Stopping training.\n",
      "Training complete. View results at: https://wandb.ai/<your-username>/nanoGPT-sentiment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: adigew (adigew-middle-east-technical-university). Use `wandb login --relogin` to force relogin\n",
      "wandb: Tracking run with wandb version 0.18.7\n",
      "wandb: Run data is saved locally in c:\\Users\\nesil.bor\\Desktop\\Folders\\master\\DI725\\DI725-transformer-sentiment-analysis\\wandb\\run-20250404_210619-t200mgfy\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run lyric-music-23\n",
      "wandb:  View project at https://wandb.ai/adigew-middle-east-technical-university/nanoGPT-sentiment\n",
      "wandb:  View run at https://wandb.ai/adigew-middle-east-technical-university/nanoGPT-sentiment/runs/t200mgfy\n",
      "wandb: - 0.007 MB of 0.007 MB uploaded\n",
      "wandb: \\ 0.008 MB of 0.008 MB uploaded\n",
      "wandb: | 0.008 MB of 0.008 MB uploaded\n",
      "wandb:                                                                                \n",
      "wandb: \n",
      "wandb: Run history:\n",
      "wandb:  backward_time █▁▁▁▁▁▁▂▁\n",
      "wandb:      data_time █▁▁▁▁▁▁▁▁\n",
      "wandb:   forward_time █▁▁▁▁▁▁▁▁\n",
      "wandb:      iteration ▁▁▂▃▃▄▅▅▅▆▆▇██\n",
      "wandb:  learning_rate ▁▅████▇▇▆\n",
      "wandb: train_accuracy ▁▅▇▇▆▆▇█▇\n",
      "wandb:     train_loss █▆▂▃▅▅▃▁▃\n",
      "wandb:   val_accuracy ▁▇███\n",
      "wandb:       val_loss █▃▁▂▃\n",
      "wandb: \n",
      "wandb: Run summary:\n",
      "wandb:  backward_time 0.11744\n",
      "wandb:      data_time 0\n",
      "wandb:   forward_time 0.00534\n",
      "wandb:      iteration 400\n",
      "wandb:  learning_rate 0.00014\n",
      "wandb: train_accuracy 0.875\n",
      "wandb:     train_loss 0.32792\n",
      "wandb:   val_accuracy 0.8875\n",
      "wandb:       val_loss 0.66436\n",
      "wandb: \n",
      "wandb:  View run lyric-music-23 at: https://wandb.ai/adigew-middle-east-technical-university/nanoGPT-sentiment/runs/t200mgfy\n",
      "wandb:  View project at: https://wandb.ai/adigew-middle-east-technical-university/nanoGPT-sentiment\n",
      "wandb: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "wandb: Find logs at: .\\wandb\\run-20250404_210619-t200mgfy\\logs\n"
     ]
    }
   ],
   "source": [
    "!python train.py config/train_sentiment.py --device=cpu --out_dir=\"out-sentiment\" --compile=False --eval_iters=20 --log_interval=50 --block_size=64 --batch_size=12 --n_layer=4 --n_head=4 --n_embd=128 --max_iters=1000 --lr_decay_iters=1000 --dropout=0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2d4052-9674-49d9-91a4-c79c887e4c93",
   "metadata": {
    "tags": []
   },
   "source": [
    "*Here, since we are running on CPU instead of GPU we must set both `--device=cpu` and also turn off PyTorch 2.0 compile with `--compile=False`. Then when we evaluate we get a bit more noisy but faster estimate (`--eval_iters=20`, down from 200), our context size is only 64 characters instead of 256, and the batch size only 12 examples per iteration, not 64. We'll also use a much smaller Transformer (4 layers, 4 heads, 128 embedding size), and decrease the number of iterations to 2000 (and correspondingly usually decay the learning rate to around max_iters with `--lr_decay_iters`). Because our network is so small we also ease down on regularization (`--dropout=0.0`). This still runs in about ~5 minutes, but gets us a loss of only 1.88 and therefore also worse samples, but it's still good fun:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b15c137-c810-495b-8962-573f7c4a7d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: sample.py [-h] [--out_dir OUT_DIR] [--checkpoint CHECKPOINT]\n",
      "sample.py: error: unrecognized arguments: --device=cpu\n"
     ]
    }
   ],
   "source": [
    "!python sample.py --out_dir=out-sentiment --device=cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75225da9-4908-4f9a-a1f6-f49ebdeb888c",
   "metadata": {},
   "source": [
    "Generates samples like this:\n",
    "\n",
    "```\n",
    "Sancho nother with this then of everantan has for five he enver any\n",
    "\n",
    "shal were than as in though they and I knight the sther his a jlage,\n",
    "\n",
    "and mad priled and squiel a hist to in feet she took and and sersse to her of\n",
    "\n",
    "Marest and good was pefor rubt some by than lave from his dintat all\n",
    "\n",
    "pack that he remants to goost ever to him arestiance of it the were to who\n",
    "\n",
    "which mom, worly gane for he sporen gort he was roosion, and be that\n",
    "\n",
    "it thou, so so he kniders what the and him of him dest us on shart\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d89c51c-d9ee-4206-879e-c39f05aed2cc",
   "metadata": {},
   "source": [
    "*Not bad for ~3 minutes on a CPU, for a hint of the right character gestalt. If you're willing to wait longer, feel free to tune the hyperparameters, increase the size of the network, the context length (`--block_size`), the length of training, etc.*\n",
    "\n",
    "*Finally, on Apple Silicon Macbooks and with a recent PyTorch version make sure to add `--device=mps` (short for \"Metal Performance Shaders\"); PyTorch then uses the on-chip GPU that can *significantly* accelerate training (2-3X) and allow you to use larger networks. See [Issue 28](https://github.com/karpathy/nanoGPT/issues/28) for more.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae6eaff-1ca1-4791-a361-b9cef9b822a0",
   "metadata": {},
   "source": [
    "## Finetuning\n",
    "\n",
    "Finetuning or transfer learning is a precious method of achieving better models thanks to pre-trained models. Finetuning GPT models is just as simple as training from scratch! We will now download the Don-Quixote (again) but this time we will define it with tokens (using OpenAI's BPE tokenizer) instead of characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a84c540c-f048-446b-8cc8-214fffdb102d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train has 592,353 tokens\n",
      "val has 66,303 tokens\n"
     ]
    }
   ],
   "source": [
    "!python data/don/prepare.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039b052b-5c7d-4741-b3d5-217966d5ddf6",
   "metadata": {},
   "source": [
    "Run an example finetuning like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d0ba8e9-3977-43fd-b98d-a37b663d6010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/finetune_don.py:\n",
      "import time\n",
      "\n",
      "out_dir = 'out-don'\n",
      "eval_interval = 5\n",
      "eval_iters = 40\n",
      "wandb_log = False # feel free to turn on\n",
      "wandb_project = 'don'\n",
      "wandb_run_name = 'ft-' + str(time.time())\n",
      "\n",
      "dataset = 'don'\n",
      "init_from = 'gpt2' # this is the GPT-2 model\n",
      "\n",
      "# only save checkpoints if the validation loss improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "# the number of examples per iter:\n",
      "batch_size = 1\n",
      "gradient_accumulation_steps = 32\n",
      "max_iters = 20\n",
      "\n",
      "# finetune at constant LR\n",
      "learning_rate = 3e-5\n",
      "decay_lr = False\n",
      "\n",
      "Overriding: compile = False\n",
      "tokens per iteration will be: 32,768\n",
      "Initializing from OpenAI GPT-2 weights: gpt2\n",
      "loading weights from pretrained gpt: gpt2\n",
      "forcing vocab_size=50257, block_size=1024, bias=True\n",
      "overriding dropout rate to 0.0\n",
      "number of parameters: 123.65M\n",
      "num decayed parameter tensors: 50, with 124,318,464 parameters\n",
      "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
      "using fused AdamW: True\n",
      "step 0: train loss 3.3541, val loss 3.3616\n",
      "iter 0: loss 3.3537, time 4283.39ms, mfu -100.00%\n",
      "iter 1: loss 3.3553, time 1665.65ms, mfu -100.00%\n",
      "iter 2: loss 3.0166, time 1627.01ms, mfu -100.00%\n",
      "iter 3: loss 3.0804, time 1658.45ms, mfu -100.00%\n",
      "iter 4: loss 3.2411, time 1671.46ms, mfu -100.00%\n",
      "step 5: train loss 3.1896, val loss 3.0898\n",
      "saving checkpoint to out-don\n",
      "iter 5: loss 2.9270, time 6360.53ms, mfu 1.41%\n",
      "iter 6: loss 3.0989, time 1661.93ms, mfu 1.81%\n",
      "iter 7: loss 3.3934, time 1657.17ms, mfu 2.17%\n",
      "iter 8: loss 2.8056, time 1669.14ms, mfu 2.49%\n",
      "iter 9: loss 3.1196, time 1648.85ms, mfu 2.79%\n",
      "step 10: train loss 3.1041, val loss 3.0200\n",
      "saving checkpoint to out-don\n",
      "iter 10: loss 3.1351, time 5963.51ms, mfu 2.66%\n",
      "iter 11: loss 3.1568, time 1658.26ms, mfu 2.94%\n",
      "iter 12: loss 3.0418, time 1654.39ms, mfu 3.19%\n",
      "iter 13: loss 3.3787, time 1660.57ms, mfu 3.41%\n",
      "iter 14: loss 3.0971, time 1681.47ms, mfu 3.60%\n",
      "step 15: train loss 3.0677, val loss 3.0510\n",
      "iter 15: loss 2.6755, time 3055.28ms, mfu 3.53%\n",
      "iter 16: loss 3.4083, time 1672.20ms, mfu 3.72%\n",
      "iter 17: loss 2.9909, time 1659.64ms, mfu 3.89%\n",
      "iter 18: loss 2.9087, time 1662.82ms, mfu 4.04%\n",
      "iter 19: loss 3.1812, time 1659.55ms, mfu 4.18%\n",
      "step 20: train loss 3.0000, val loss 3.0810\n",
      "iter 20: loss 3.1975, time 3088.11ms, mfu 4.05%\n"
     ]
    }
   ],
   "source": [
    "!python train.py config/finetune_don.py --compile=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9040fa6b-64a7-4964-afd5-3c5c545aeaba",
   "metadata": {},
   "source": [
    "This will load the config parameter overrides in `config/finetune_don.py`. Basically, we initialize from a GPT2 checkpoint with `init_from` and train as normal, except shorter and with a small learning rate. Model architecture is changable to `{'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}`) and can be decreased in size by the `block_size` (context length). The best checkpoint (lowest validation loss) will be in the `out_dir` directory, e.g. in `out-don` by default, per the config file. You can then run the code in `sample.py --out_dir=out-don`:\n",
    "```\n",
    "* * All creatures that enter the world below may so far as want to observe the rules of their own land, and may obey them under the hand of their lord, and may not follow others below.\n",
    "\n",
    "* * *\n",
    "\n",
    "THE PORT COLLIDATES,\n",
    "\n",
    "- * *\n",
    "\n",
    "ON the light, and the light to the dark, and the darkness to the light, and the darkness to the darkness, were the present-day laws of monarchy, whose lordship they approved in their faces and hearts. From this moment on, however, they had no other representation to give than that of their master, who, for all that was said or heard, had reached the height of his power.\n",
    "\n",
    "The king's hand, though at times little more than a finger of his, required no more than a finger of his, and that power was, that of holding his eye, and the other of his, in his own, body.\n",
    "\n",
    "When this was spoken of, it was a simple and noble quibble, and the subject of this was so as to admit of the few who had any forsemination, and the few who had the most to go on.\n",
    "\n",
    "The time did not come for a thought of this, and for a moment the very thought of it seemed to fall to the ground.\n",
    "\n",
    "But that thought did not come to pass; though the king was not speaking of the king, it came to pass that the king, with all his might, and all his cunning, and no other sense, and without any understanding, and without any desire for the utmost of his services, and without any desire to put an end to his own glory, and without any desire to hide his triumph, had found the time to say that this was what he thought on the subject of religion; that it was what he thought, and according as it seemed to him to be as good or better to him than to the other kings, and he was in no sense a king, for it seemed to him he could never have any more power than he had to be; that it was a matter of his will and power; and that it was all a matter of his will, for he was determined that this look and that to which he might have been given to hold it was the best in himself.\n",
    "\n",
    "And so it was that the king, who was all around him, and all around him; and so\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0037197e-f7e3-40bc-80fd-e2626cf4dc4a",
   "metadata": {},
   "source": [
    "# Inference and Sampling\n",
    "Use the script `sample.py` to sample either from pre-trained GPT-2 models released by OpenAI, or from a model you trained yourself. For example, here is a way to sample from the largest available `gpt2-xl` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83d42305-ff4f-4153-be5c-3067e22ccf2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding: out_dir = out-don\n",
      "Overriding: start = Explain the relationship between Don Quixote and Sancho Panza\n",
      "Overriding: num_samples = 5\n",
      "Overriding: max_new_tokens = 100\n",
      "number of parameters: 123.65M\n",
      "No meta.pkl found, assuming GPT-2 encodings...\n",
      "Explain the relationship between Don Quixote and Sancho Panza. The story, according to the newspapers, is that the two men want to take over the city, and that Sancho has accepted that.\n",
      "\n",
      "I have been told that Don Quixote was not the first man to arrive to the city, and that Sancho was not the first. I have no reason to doubt that, for Don Quixote is an itinerant man, who is always crossing the country, and with a lot of money. I have been told that many other\n",
      "---------------\n",
      "Explain the relationship between Don Quixote and Sancho Panza, the magician who is almost always mistaken for the Countess of Sancho Panza. This is a trick used by the Countess of Sancho Panza.\n",
      "\n",
      "The Countess of Sancho Panza is made the subject of many rumors, but the truth is that since she is the same as the Countess of Sancho Panza, Sancho Panza's presence on earth is not disguised. No, she is not disguised, for she is the person who will become the subject\n",
      "---------------\n",
      "Explain the relationship between Don Quixote and Sancho Panza\n",
      "\n",
      "\n",
      "Don's obsession with his own career was not just a matter of money but of ambition. He was the son of the Sancho Panza family, the family which had been founded in a humble village in the San Diego mountains during the time that it was the 'Calavera of the San Diego Basin' and had lived there for nearly half a century.\n",
      "\n",
      "\n",
      "But it was the family that had gone missing on the night of May 9, 1833, leaving Don Quix\n",
      "---------------\n",
      "Explain the relationship between Don Quixote and Sancho Panza! The first time I saw Sancho, and I was the first to introduce him to some of his most notable friends, I had a good time, full of interest and interest. He is a very successful editor, owing to his good taste, with a good wit and fair, lively story. I came to know him over several years ago on the island of Puerto Rico, where I was to visit, and, as I am to say, we have enough copies to give to you and your\n",
      "---------------\n",
      "Explain the relationship between Don Quixote and Sancho Panza. The latter's speech has in it a request for his aid against the misfortunes of the Negros, who are seeking to be buried, and in a later line has the subject of the misfortunes of the Santa Ana people, who are wanting to be buried, where they are to be buried, and the latter having been buried, the people in want of money, and the Santa Ana amiss, want some of the money to be used for the building of a chamber\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "!python sample.py --out_dir=out-don --start=\"Explain the relationship between Don Quixote and Sancho Panza\" --num_samples=5 --max_new_tokens=100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9543a34c-4311-481c-8754-e338afbd46de",
   "metadata": {},
   "source": [
    "If you'd like to sample from a model you trained, use the `--out_dir` to point the code appropriately. You can also prompt the model with some text from a file, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24f20da7-fc90-4dbf-b860-8385a4323b79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding: start = FILE:prompt/fictional.txt\n",
      "Overriding: out_dir = out-don\n",
      "Overriding: num_samples = 1\n",
      "Overriding: max_new_tokens = 100\n",
      "number of parameters: 123.65M\n",
      "No meta.pkl found, assuming GPT-2 encodings...\n",
      "Dain charges head on with his warhammer and full plate clad armor. Determined to topple anything in front of him.\n",
      "\n",
      "One of the last bastions of the Dawn so far held in the voids held a massive wattle and rotted horse that had been drenched in sweat and had been stripped of its armour. Its bones were cast into the ground and then buried in the snow.\n",
      "\n",
      "The dragon charged and made a wry noise, and turned back to face the Dawn. He glanced at the dragon, and then at Dain with a smile that was almost full of fear, but that made\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "!python sample.py --start=FILE:\"prompt/fictional.txt\" --out_dir=\"out-don\" --num_samples=1 --max_new_tokens=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6b1f055-0b1e-4f76-8311-2dd0df84eda5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding: start = FILE:prompt/positive_review.txt\n",
      "Overriding: out_dir = out-don\n",
      "Overriding: num_samples = 1\n",
      "Overriding: max_new_tokens = 500\n",
      "number of parameters: 123.65M\n",
      "No meta.pkl found, assuming GPT-2 encodings...\n",
      "This place was DELICIOUS!! My parents saw a recommendation to visit this place from Rick Sebak's \\\"25 Things I Like About Pittsburgh\\\" and he's usually pretty accurate. His recommendations were to try the Reuben, Fish Sandwich and Open-Faced Steak Sandwich. We went early afternoon for a late lunch today (a Saturday) and were seated right away. The staff is extremely friendly. My Mom & I each had the fish sandwich, while my Dad & Brother had a Reuben sandwich. The fish was very good, but the Reuben was to die for! Both dishes were massive, and could very easily be shared between two people. On top of being extremely large portions, it was incredibly affordable. The giant fish sandwich was $8 and the giant Reuben was $7.50. Our drinks were always filled and we were checked on several times during the meal. We will definitely be back!!! Oh and a bit of advice ahead of time - they take CASH ONLY. So come prepared, but I'm pretty sure I saw an ATM there as well. And I do believe they are closed on Sundays & Mondays. Have fun and see you around!\n",
      "\n",
      "I've so far loved this spot and am going to be returning soon. I was coming up here looking to try some of the good things they have on offer. I was really happy to visit the steak sandwich and it had a definite hit of flavor. I was going to visit the steak man and the salad and the beer as well. And I don't know what else I want to see here, but I cannot wait.\n",
      "\n",
      "Honey, we don't like steak here and I know there's not a lot to say more. This was a great way to spend this afternoon at an amazing spot for a nice and iced meal. I was looking to find a place to eat for a quick meal. I had a quick breakfast and I tried the steak man's salad which was a nice side dish and was pretty good. I ordered the beer and the wine, and the steak sandwich was my favorite. I don't go to this place because it's a bit overpriced, but I can say it has a good name and this was a time to try it out. I had this for dinner last night, and I can't say I don't go there sometimes, especially with that kind of food. I thought it was a great spot to be staying for a while, considering the portions.\n",
      "\n",
      "I haven't had a steak sandwich in awhile but I have to say that when I came up through the restaurant it was a little gross. This restaurant has all kinds of great and juicy things at it's prices. I mean they were so good I tried everything. But I go first thing in the morning and I get sandwiches, fish, and salad. I got the fish sandwich with a salad. I got the steak sandwich with a salad. And finally the steak sandwich. I am so happy I found this place because I am such a sucker for deliciousness and fresh ingredients. Because as a chef I am so thankful and I know the restaurant is worthy of a nice restaurant. And my family and friends are so excited to go and see this place as well, so I'm convinced it would be a dream come true and would be a good place to eat a nice steak sandwich if you are pretty new to the dining look out for. That's what I mean. I don't know if I will come back. I know I just may come back, but I don't know what will happen to me or my family.\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "!python sample.py --start=FILE:\"prompt/positive_review.txt\" --out_dir=\"out-don\" --num_samples=1 --max_new_tokens=500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2600147b-8b88-4de0-bc2c-ee2fb7e781d1",
   "metadata": {},
   "source": [
    "I hope you will enjoy with the GPT as much as I did!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9c0355-3e89-4ea0-9976-411dc15d76e5",
   "metadata": {},
   "source": [
    "## Efficiency notes\n",
    "\n",
    "*For simple model benchmarking and profiling, `bench.py` might be useful. It's identical to what happens in the meat of the training loop of `train.py`, but omits much of the other complexities.*\n",
    "\n",
    "*Note that the code by default uses [PyTorch 2.0](https://pytorch.org/get-started/pytorch-2.0/). At the time of writing (Dec 29, 2022) this makes `torch.compile()` available in the nightly release. The improvement from the one line of code is noticeable, e.g. cutting down iteration time from ~250ms / iter to 135ms / iter. Nice work PyTorch team!*\n",
    "\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "*Note that by default this repo uses PyTorch 2.0 (i.e. `torch.compile`). This is fairly new and experimental, and not yet available on all platforms (e.g. Windows). If you're running into related error messages try to disable this by adding `--compile=False` flag. This will slow down the code but at least it will run.*\n",
    "\n",
    "*For some context on this repository, GPT, and language modeling it might be helpful to watch [Zero To Hero series](https://karpathy.ai/zero-to-hero.html). Specifically, the [GPT video](https://www.youtube.com/watch?v=kCc8FmEb1nY) is popular if you have some prior language modeling context.*\n",
    "\n",
    "## Acknowledgements\n",
    "\n",
    "This code is a fork from Andrej Karpathy's introductory [NanoGPT repository](https://github.com/karpathy/nanoGPT), which is an updated form of minGPT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d4e757-8a74-40bf-a36c-9b5f59135121",
   "metadata": {},
   "source": [
    "# Further Experiments\n",
    "\n",
    "(Optional)\n",
    "\n",
    "For further experiments, you can, for example, reproduce the GPT-2, which is still powerful, by following the link to the Andrej Karpathy's repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6ce869-04d3-4278-a449-a0c8edb1807b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
