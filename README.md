# DI 725: Transformers and Attention-Based Deep Networks


The objective of this assignment is to increase familiarity with transformer architecture and various adaptations for different applications. Our task is to analyze the sentiment of customer service conversations. We will begin with the dataset and apply explanatory data analysis, extract useful information about the data, and apply pre-processing steps necessary for the task.

You can find a starter code that is a modified nanoGPT implementation here(https://github.com/caglarmert/DI725/tree/main/assignment_1). The purpose of the starter code is to get you started on the transformer implementation. After understanding the codebase and the associated starter code we will be implementing our first working example. We will apply this first model for sentiment analysis after training. For the second model, we will obtain the pre-trained model weights of gpt-2 and fine-tune it. Note that changing the model structure too much will render you unable to fine-tune over a pre-trained gpt-2 weights. You can follow the Starter_Code.

